---
title: "Central Limit Theorem:<br>The Math"
author: "Author: Jill E. Thomley"
date: '`r format(Sys.time(), "%B %d, %Y @ %I:%M %p")`'
output: 
  ioslides_presentation:
    logo: images/logoASU.jpg
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  comment = NA
)
library(tidyverse)
```


## Before We Begin...

These slides are not meant to be standalone information. You should take notes to flesh out the contents. I recommend that you create an R Markdown document where you can combine information and code from the slides and your own additional notes and explorations to make connections.

**Related Materials**

* Ch 2 of *Mathematical Statistics with Resampling and R, 2^nd^ Ed.*
* [Chapter 7](https://moderndive.com/7-sampling.html) of *Modern Dive*
* [Normal (Gaussian) Distribution: The Math ](https://stat-jet-asu.github.io/StatisticalDataAnalysis1/Slides/NormalDistributionTheory.html) class slides
* [Normal (Gaussian) Distribution: `R` Code ](https://stat-jet-asu.github.io/StatisticalDataAnalysis1/Slides/NormalDistributionRCode.html) class slides


## The Normal Model

$$X \sim N(\mu,\sigma^2)$$

$x$ = {-$\infty$ to +$\infty$} 

$\mu$ = the mean of the distribution  

$\sigma^2$ = the variance of the distribution  

$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/(2\sigma^2)}$$

$$F(x) = \text{no closed-form solution}$$

$$E[X] = \mu \text{ and }Var[X] = \sigma^2$$


## Sampling and Central Limit Theorem

Suppose we select a reasonably large sample of $i.i.d.$ random variables $X_1, X_2, ..., X_n$ (i.e., sample from the population $X$).

Then, according to the central limit theorem (CLT), the sampling distribution of the sample mean will be approximately normal, regardless of the underlying population distribution. ([proof](https://towardsdatascience.com/central-limit-theorem-proofs-actually-working-through-the-math-a994cd582b33))

This rule also applies to the sums of sample values, as well as to the count or proportion of successes in sample from a binomial population. We can algebraically re-express the CLT in terms of these commonly used cases.

**Note:** $i.i.d.$ = [independent and identically distributed](https://towardsdatascience.com/independent-and-identically-distributed-ce250ad1bfa8) (each random variable has the same probability distribution and all are mutually independent)


## Why Central Limit Theorem?

* The term central limit theorem first appeared in a 1920 paper by Hungarian mathematician George Pólya (1887-1985). 

* He used the term "zentral" (central) to indicate the theorem's central importance to statistical theory. 

* Many consider the CLT and the law of large numbers to be the two foundational theorems in probability and statistics.

* The law of large numbers says that, as the number of $i.i.d.$ randomly generated variables increases, their sample mean approaches their theoretical mean. ([proof](https://towardsdatascience.com/proof-of-the-law-of-large-numbers-part-1-the-weak-law-daf412178d3a)) ([proof](https://towardsdatascience.com/proof-of-the-law-of-large-numbers-part-2-the-strong-law-356aa608ca5d))

* The first known proof of the law of large numbers appears in the 1713 book [*Ars Conjectandi*](https://books.google.com/books?id=XPOf7STJ3y4C&pg=PP5#v=onepage&q&f=false) by Swiss mathematician Jacob Bernoulli (1654-1705), posthumously published.


## Sampling Distribution --- Mean $\bar{x}$

Let $X$ be a population with finite mean $\mu$ and variance $\sigma^2$ (and standard deviation $\sigma$). 

For a sufficiently large sample size $n$, the sampling distribution of the sample mean $\bar{x}$ is:

* approximately normal

* has a mean $\mu_{\bar{x}}$ = $\mu$

* has a variance $\sigma^2_{\bar{x}}$ = $\frac{\sigma^2}{n}$

* has a standard error $\sigma_{\bar{x}}$ = $\sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}$


## From Mean to Sum

When we calculate a sample mean, we first take the sum of all sample values and then divide by the sample size.

$$\bar{x} = \frac{\sum_1^n{x_i}}{n}$$

Conversely, if we know the sample mean, we can find the sum (or total) by multiplying $\bar{x}$ by $n$.

$$n\bar{x} = \sum_1^n{x_i}$$

Since $n$ is a constant, these are simple linear transformations. If $\bar{x}$ is approximately normal, the sum will be as well, just rescaled.


## Sampling Distribution --- Sum $\sum_1^n{x_i}$

Let $X$ be a population with finite mean $\mu$ and variance $\sigma^2$ (and standard deviation $\sigma$). 

For a sufficiently large sample size $n$, the sampling distribution of the sample sum $\sum_1^n{x_i} = n\bar{x}$ is:

* approximately normal

* has a mean $\mu_{sum}$ = $n\mu$

* has a variance $\sigma^2_{sum}$ = $n\sigma^2$

* has a standard error $\sigma_{sum}$ = $\sqrt{n\sigma^2} = \sqrt{n}\sigma$


## What is sufficiently large?

A commonly used rule of thumb is that "sufficiently large" is a sample with $n \ge 30$. However, in practice...

* when $X$ has a normal distribution, the sampling distribution of the mean is exactly normal for all $n$.

* if the underlying distribution is symmetric and unimodal with no outliers (e.g., uniform), then $n < 30$ is often sufficient.

* if the underlying distribution is moderately (or more) skewed or has outliers (e.g., exponential), $n >> 30$ may be required.

* If the sample size is not large enough, then expected value and variance are as shown in the CLT, but the shape will be indeterminate (dependent on the population shape).


## Sampling Distribution --- Success Count

Under certain circumstances, we can approximate the binomial distribution using the normal distribution. In that case, $\mu = np$ (binomial mean) and $\sigma^2 = np(1-p)$ (binomial variance). If we have a sufficiently large sample size $n$, the sampling distribution of the number of successes $x$ is:

* approximately normal

* has a mean $\mu_x$ = $np$

* has a variance $\sigma^2_x$ = $np(1-p)$

* has a standard error $\sigma_x$ = $\sqrt{np(1-p)}$


## From Count to Proportion

When we calculate a sample proportion of successes, we take the count of successes $x$ and divide by $n$ (number of trials).

$$\hat{p} = \frac{x}{n}$$

Just like with the sample mean and sum, since $n$ is a constant, this is a simple linear transformation. If count of successes is approximately normal, the proportion will be too, just rescaled. In this case, $\mu = p$ and $\sigma^2 = \frac{p(1-p)}{n}$.

We can also think of this as a kind of mean. Suppose successes are represented by $1$s and failures are represented by $0$s in the sample. Algebraically, calculating the proportion $\hat{p}$ is the same as finding the mean of the sample of $0$s and $1$s. 


## Sampling Distribution --- Proportion $\hat{p}$

Let X be a binomial population with parameters $n$ and $p$. This is a particular case of the CLT, which works for *any* population.

If we have a sufficiently large sample size $n$, the sampling distribution of the proportion of successes $\hat{p}$ is:

* approximately normal

* has a mean $\mu_\hat{p}$ = $p$

* has a variance $\sigma^2_\hat{p}$ = $\frac{p(1-p)}{n}$

* has a standard error $\sigma_\hat{p}$ = $\sqrt{\frac{p(1-p)}{n}}$


## What is sufficiently large?

For proportion data from a binomial process with probability $p$, a commonly used rule of thumb is that "sufficiently large" is a sample where $np \ge 10$ *and* $n(1 – p) \ge 10$ (i.e., more than 10 successes and failures in the sample). However...

* some sources say $np \ge 5$ and $n(1 – p) \ge 5$ is sufficient for approximate normality, while others say $\ge 15$. As we have seen, approximate normality is a judgment.

* in general, the farther $p$ is from $0.5$, the more skewed the binomial distribution will be; thus, larger sample sizes are required when $p$ is closer to $0$ or $1$ than when it is $0.5$.

**Note:** We have to estimate $p$ from the sample, so we use $\hat{p}$.
